from google.adk.tools import FunctionTool
from google.adk.tools.tool_context import ToolContext
from google.adk.agents import LlmAgent
import asyncio
import websockets
import json
import base64
import io
from typing import Dict, Any, Optional, List, Set
import os
import sys
from datetime import datetime
import uuid
import math
import random

# Global server reference for proper shutdown
websocket_server = None
server_task = None

# Load .env file FIRST
import dotenv
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
env_path = os.path.join(project_root, '.env')
dotenv.load_dotenv(env_path)

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(project_root)

# å¯¼å…¥é…ç½®
try:
    from config import get_gemini_api_key, get_audio_config, set_gemini_api_key
except ImportError:
    # å¦‚æœæ‰¾ä¸åˆ°configæ¨¡å—ï¼Œæä¾›é»˜è®¤å®ç° (.env already loaded above)
    def get_gemini_api_key():
        return os.getenv('GEMINI_API_KEY') or os.getenv('GOOGLE_AI_API_KEY')
    
    def get_audio_config():
        return {
            "model": "gemini-2.5-flash-preview-tts",
            "default_language": "en-US",
            "websocket_port": 8765
        }


class AudioProcessor:
    """éŸ³é¢‘å¤„ç†ç±»ï¼Œè´Ÿè´£Gemini TTSå’ŒéŸ³é¢‘æµç®¡ç†"""
    
    def __init__(self):
        # ä¸å†ä½¿ç”¨ä»»ä½•Google Cloud TTSç›¸å…³ä»£ç 
        self.connected_clients: Set = set()
        self.audio_queue = asyncio.Queue()
        
        # ä»é…ç½®æ–‡ä»¶è·å–è®¾ç½®
        self.config = get_audio_config()
        
        # Geminié…ç½®
        self.gemini_model = self.config["model"]
        
        print(f"ğŸ¯ Audio Processoråˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {self.gemini_model}")


# å…¨å±€éŸ³é¢‘å¤„ç†å™¨å®ä¾‹
audio_processor = AudioProcessor()


async def text_to_speech(
    tool_context: Optional[ToolContext] = None,
    text: str = "", 
    voice_style: str = "enthusiastic",
    language: str = "en-US"
) -> Dict[str, Any]:
    """
    ä½¿ç”¨çœŸæ­£çš„ Gemini TTS å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯­éŸ³
    
    Args:
        tool_context: ADKå·¥å…·ä¸Šä¸‹æ–‡
        text: éœ€è¦è½¬æ¢çš„è§£è¯´æ–‡æœ¬
        voice_style: è¯­éŸ³é£æ ¼ (enthusiastic, calm, dramatic)
        language: è¯­è¨€ä»£ç  (en-US, en-CAç­‰)
        
    Returns:
        åŒ…å«éŸ³é¢‘ä¿¡æ¯å’ŒçŠ¶æ€çš„å­—å…¸
    """
    try:
        print(f"ğŸ™ï¸ Gemini TTS: å¼€å§‹è½¬æ¢ - {text[:50]}...")
        
        # æ£€æŸ¥API Key
        api_key = get_gemini_api_key()
        if not api_key:
            error_msg = "æœªæ‰¾åˆ°Gemini API Keyï¼Œè¯·è®¾ç½®GEMINI_API_KEYç¯å¢ƒå˜é‡"
            print(f"âŒ {error_msg}")
            
            if tool_context:
                tool_context.state["last_audio_generation"] = {
                    "status": "error",
                    "error": error_msg,
                    "model": "none"
                }
            
            return {
                "status": "error",
                "error": error_msg,
                "text": text[:50] + "..." if len(text) > 50 else text,
                "model": "none"
            }
        
        # å°è¯•ä½¿ç”¨çœŸæ­£çš„ Gemini TTS
        try:
            from google import genai
            from google.genai import types
            
            # åˆ›å»ºå®¢æˆ·ç«¯
            client = genai.Client(api_key=api_key)
            
            # æ ¹æ®è¯­éŸ³é£æ ¼é€‰æ‹©å£°éŸ³
            voice_mapping = {
                "enthusiastic": "Puck",      # å…´å¥‹çš„å£°éŸ³
                "dramatic": "Kore",          # æˆå‰§æ€§çš„å£°éŸ³
                "calm": "Aoede"             # å¹³é™çš„å£°éŸ³
            }
            
            voice_name = voice_mapping.get(voice_style, "Puck")
            
            # æ„å»ºæç¤ºè¯
            if voice_style == "enthusiastic":
                prompt = f"Say with high energy and excitement like a sports announcer: {text}"
            elif voice_style == "dramatic":
                prompt = f"Say with dramatic intensity and emphasis: {text}"
            elif voice_style == "calm":
                prompt = f"Say in a calm, professional announcer voice: {text}"
            else:
                prompt = f"Say clearly: {text}"
            
            print(f"ğŸ”Š ä½¿ç”¨å£°éŸ³: {voice_name}, é£æ ¼: {voice_style}")
            
            # è°ƒç”¨ Gemini TTS API
            response = client.models.generate_content(
                model="gemini-2.5-flash-preview-tts",
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_modalities=["AUDIO"],
                    speech_config=types.SpeechConfig(
                        voice_config=types.VoiceConfig(
                            prebuilt_voice_config=types.PrebuiltVoiceConfig(
                                voice_name=voice_name
                            )
                        )
                    )
                )
            )
            
            # è·å–éŸ³é¢‘æ•°æ®
            audio_data = response.candidates[0].content.parts[0].inline_data.data
            
            # ç”ŸæˆéŸ³é¢‘ID
            audio_id = str(uuid.uuid4())[:8]
            timestamp = datetime.now().strftime("%H%M%S")
            
            print(f"âœ… çœŸå®Gemini TTSæˆåŠŸ! å¤§å°: {len(audio_data):,} å­—èŠ‚")
            
            # ç¼–ç éŸ³é¢‘æ•°æ®
            audio_base64 = base64.b64encode(audio_data).decode('utf-8')
            
            # å‡†å¤‡WebSocketå¹¿æ’­æ•°æ®
            broadcast_data = {
                "type": "audio_stream",
                "audio_id": audio_id,
                "text": text,
                "voice_style": voice_style,
                "voice_name": voice_name,
                "timestamp": timestamp,
                "audio_data": audio_base64,
                "format": "wav",
                "model": "gemini-2.5-flash-preview-tts",
                "is_real_tts": True,
                "api_key_status": "configured"
            }
            
            # å¹¿æ’­éŸ³é¢‘
            asyncio.create_task(_broadcast_audio(broadcast_data))
            
            # æ›´æ–°å·¥å…·ä¸Šä¸‹æ–‡
            if tool_context:
                if "audio_history" not in tool_context.state:
                    tool_context.state["audio_history"] = []
                
                tool_context.state["audio_history"].append({
                    "audio_id": audio_id,
                    "text": text[:100],
                    "timestamp": timestamp,
                    "voice_style": voice_style,
                    "voice_name": voice_name,
                    "model": "gemini-2.5-flash-preview-tts",
                    "is_real_tts": True
                })
                
                tool_context.state["last_audio_generation"] = {
                    "status": "success",
                    "audio_id": audio_id,
                    "duration_estimate": len(text) * 0.05,
                    "model": "gemini-2.5-flash-preview-tts",
                    "is_real_tts": True
                }
            
            return {
                "status": "success",
                "audio_id": audio_id,
                "text_length": len(text),
                "voice_style": voice_style,
                "voice_name": voice_name,
                "language": language,
                "timestamp": timestamp,
                "model": "gemini-2.5-flash-preview-tts",
                "is_real_tts": True,
                "audio_size": len(audio_data),
                "audio_data": audio_base64,  # ç›´æ¥è¿”å›éŸ³é¢‘æ•°æ®
                "message": f"çœŸå®Gemini TTSéŸ³é¢‘ç”ŸæˆæˆåŠŸï¼ŒID: {audio_id}"
            }
            
        except ImportError:
            error_msg = "google-genaiåº“æœªå®‰è£…ï¼Œè¯·å®‰è£…: pip install google-genai"
            print(f"âŒ {error_msg}")
            
            if tool_context:
                tool_context.state["last_audio_generation"] = {
                    "status": "error",
                    "error": error_msg,
                    "model": "none"
                }
            
            return {
                "status": "error",
                "error": error_msg,
                "text": text[:50] + "..." if len(text) > 50 else text,
                "model": "none"
            }
            
        except Exception as e:
            error_msg = f"Gemini TTS APIè°ƒç”¨å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            
            if tool_context:
                tool_context.state["last_audio_generation"] = {
                    "status": "error",
                    "error": error_msg,
                    "model": "gemini-api-error"
                }
            
            return {
                "status": "error",
                "error": error_msg,
                "text": text[:50] + "..." if len(text) > 50 else text,
                "model": "gemini-api-error"
            }
        
    except Exception as e:
        error_msg = f"éŸ³é¢‘ç”Ÿæˆå¤±è´¥: {str(e)}"
        print(f"âŒ {error_msg}")
        
        if tool_context:
            tool_context.state["last_audio_generation"] = {
                "status": "error",
                "error": error_msg,
                "model": "unknown-error"
            }
        
        return {
            "status": "error",
            "error": error_msg,
            "text": text[:50] + "..." if len(text) > 50 else text,
            "model": "unknown-error"
        }


async def _generate_fallback_audio(text: str, voice_style: str, tool_context: Optional[ToolContext] = None) -> Dict[str, Any]:
    """ç”Ÿæˆæ¨¡æ‹ŸéŸ³é¢‘ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ"""
    
    # ç”ŸæˆéŸ³é¢‘ID
    audio_id = str(uuid.uuid4())[:8]
    timestamp = datetime.now().strftime("%H%M%S")
    
    print(f"ğŸ”„ ç”Ÿæˆæ¨¡æ‹ŸéŸ³é¢‘ (å¤‡ç”¨æ–¹æ¡ˆ)")
    
    try:
        # ä½¿ç”¨æ”¹è¿›çš„è¯­éŸ³æ¨¡æ‹Ÿæ–¹æ³•
        audio_data = _generate_simple_wav_audio(text, voice_style)
        audio_base64 = base64.b64encode(audio_data).decode('utf-8')
        
        # å‡†å¤‡WebSocketå¹¿æ’­æ•°æ®
        broadcast_data = {
            "type": "audio_stream",
            "audio_id": audio_id,
            "text": text,
            "voice_style": voice_style,
            "timestamp": timestamp,
            "audio_data": audio_base64,
            "format": "wav",
            "model": "fallback_simulation",
            "is_real_tts": False,
            "api_key_status": "fallback"
        }
        
        # å¹¿æ’­éŸ³é¢‘
        asyncio.create_task(_broadcast_audio(broadcast_data))
        
        # æ›´æ–°å·¥å…·ä¸Šä¸‹æ–‡
        if tool_context:
            if "audio_history" not in tool_context.state:
                tool_context.state["audio_history"] = []
            
            tool_context.state["audio_history"].append({
                "audio_id": audio_id,
                "text": text[:100],
                "timestamp": timestamp,
                "voice_style": voice_style,
                "model": "fallback_simulation",
                "is_real_tts": False
            })
            
            tool_context.state["last_audio_generation"] = {
                "status": "success",
                "audio_id": audio_id,
                "duration_estimate": len(text) * 0.05,
                "model": "fallback_simulation",
                "is_real_tts": False
            }
        
        return {
            "status": "success",
            "audio_id": audio_id,
            "text_length": len(text),
            "voice_style": voice_style,
            "timestamp": timestamp,
            "model": "fallback_simulation",
            "is_real_tts": False,
            "audio_size": len(audio_data),
            "message": f"æ¨¡æ‹ŸéŸ³é¢‘ç”ŸæˆæˆåŠŸ (å¤‡ç”¨æ–¹æ¡ˆ)ï¼ŒID: {audio_id}"
        }
        
    except Exception as e:
        error_msg = f"æ¨¡æ‹ŸéŸ³é¢‘ç”Ÿæˆå¤±è´¥: {str(e)}"
        print(f"âŒ {error_msg}")
        
        return {
            "status": "error",
            "error": error_msg,
            "text": text[:50] + "..." if len(text) > 50 else text,
            "model": "fallback"
        }


def stream_audio_websocket(
    tool_context: Optional[ToolContext] = None,
    port: int = 8765,
    host: str = "localhost"
) -> Dict[str, Any]:
    """
    å¯åŠ¨WebSocketæœåŠ¡å™¨è¿›è¡ŒéŸ³é¢‘æµä¼ è¾“
    """
    try:
        print(f"ğŸŒ å¯åŠ¨WebSocketéŸ³é¢‘æµæœåŠ¡å™¨ {host}:{port}")
        
        # å¯åŠ¨WebSocketæœåŠ¡å™¨
        global server_task
        server_task = asyncio.create_task(_start_websocket_server(host, port))
        
        if tool_context:
            tool_context.state["websocket_server"] = {
                "status": "running",
                "host": host,
                "port": port,
                "started_at": datetime.now().isoformat()
            }
        
        return {
            "status": "success",
            "message": f"WebSocketéŸ³é¢‘æµæœåŠ¡å™¨å·²å¯åŠ¨",
            "server_url": f"ws://{host}:{port}",
            "port": port,
            "host": host
        }
        
    except Exception as e:
        error_msg = f"WebSocketæœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {str(e)}"
        print(f"âŒ {error_msg}")
        
        if tool_context:
            tool_context.state["websocket_server"] = {
                "status": "error",
                "error": error_msg
            }
        
        return {
            "status": "error",
            "error": error_msg
        }


def get_audio_status(tool_context: Optional[ToolContext] = None) -> Dict[str, Any]:
    """è·å–éŸ³é¢‘ä»£ç†çŠ¶æ€"""
    try:
        status_info = {
            "connected_clients": len(audio_processor.connected_clients),
            "queue_size": audio_processor.audio_queue.qsize(),
            "model": audio_processor.gemini_model,
            "api_key_configured": bool(get_gemini_api_key()),
            "timestamp": datetime.now().isoformat()
        }
        
        if tool_context:
            audio_history = tool_context.state.get("audio_history", [])
            last_generation = tool_context.state.get("last_audio_generation", {})
            websocket_status = tool_context.state.get("websocket_server", {})
            
            status_info.update({
                "audio_history_count": len(audio_history),
                "last_generation": last_generation,
                "websocket_server": websocket_status,
                "recent_audio": audio_history[-3:] if len(audio_history) > 3 else audio_history
            })
        
        return {
            "status": "success",
            "audio_agent_status": status_info
        }
        
    except Exception as e:
        return {
            "status": "error",
            "error": f"è·å–çŠ¶æ€å¤±è´¥: {str(e)}"
        }


# è¾…åŠ©å‡½æ•°
def _apply_voice_style_text(text: str, style: str) -> str:
    """æ ¹æ®é£æ ¼è°ƒæ•´æ–‡æœ¬å†…å®¹"""
    if style == "enthusiastic":
        return f"{text}!" if not text.endswith(('!', '?', '.')) else text
    elif style == "dramatic":
        return f"*{text}*" if not text.startswith('*') else text
    elif style == "calm":
        return text.rstrip('!') + '.' if text.endswith('!') else text
    else:
        return text


def _get_speaking_rate(style: str) -> float:
    """æ ¹æ®é£æ ¼è®¾ç½®è¯­é€Ÿ"""
    config = get_audio_config()
    return config.get("speaking_rates", {}).get(style, 1.1)


def _get_pitch(style: str) -> float:
    """æ ¹æ®é£æ ¼è®¾ç½®éŸ³è°ƒ"""
    config = get_audio_config()
    return config.get("pitch_adjustments", {}).get(style, 0.0)


async def _broadcast_audio(data: Dict[str, Any]):
    """å‘æ‰€æœ‰è¿æ¥çš„å®¢æˆ·ç«¯å¹¿æ’­éŸ³é¢‘æ•°æ®"""
    if not audio_processor.connected_clients:
        print("ğŸ“¡ æ²¡æœ‰è¿æ¥çš„å®¢æˆ·ç«¯ï¼Œè·³è¿‡å¹¿æ’­")
        return
    
    message = json.dumps(data)
    disconnected_clients = set()
    
    for client in audio_processor.connected_clients:
        try:
            await client.send(message)
            print(f"ğŸ“¤ éŸ³é¢‘å·²å‘é€åˆ°å®¢æˆ·ç«¯")
        except websockets.exceptions.ConnectionClosed:
            disconnected_clients.add(client)
        except Exception as e:
            print(f"âŒ å¹¿æ’­å¤±è´¥: {e}")
            disconnected_clients.add(client)
    
    # æ¸…ç†æ–­å¼€çš„è¿æ¥
    for client in disconnected_clients:
        audio_processor.connected_clients.remove(client)


async def _start_websocket_server(host: str, port: int):
    """å¯åŠ¨WebSocketæœåŠ¡å™¨"""
    global websocket_server
    
    async def handle_client(websocket):
        print(f"ğŸ”— æ–°å®¢æˆ·ç«¯è¿æ¥: {websocket.remote_address}")
        audio_processor.connected_clients.add(websocket)
        
        try:
            # å‘é€æ¬¢è¿æ¶ˆæ¯
            welcome_msg = {
                "type": "connection",
                "status": "connected",
                "message": "æ¬¢è¿è¿æ¥åˆ°NHL GeminiéŸ³é¢‘æµ",
                "model": audio_processor.gemini_model,
                "timestamp": datetime.now().isoformat()
            }
            await websocket.send(json.dumps(welcome_msg))
            
            # ä¿æŒè¿æ¥
            async for message in websocket:
                try:
                    data = json.loads(message)
                    await _handle_client_message(websocket, data)
                except json.JSONDecodeError:
                    await websocket.send(json.dumps({
                        "type": "error",
                        "message": "æ— æ•ˆçš„JSONæ ¼å¼"
                    }))
                    
        except websockets.exceptions.ConnectionClosed:
            print(f"ğŸ”Œ å®¢æˆ·ç«¯æ–­å¼€è¿æ¥: {websocket.remote_address}")
        except Exception as e:
            print(f"âŒ WebSocketå¤„ç†é”™è¯¯: {e}")
        finally:
            audio_processor.connected_clients.discard(websocket)
    
    try:
        websocket_server = await websockets.serve(handle_client, host, port)
        print(f"ğŸš€ WebSocketéŸ³é¢‘æœåŠ¡å™¨è¿è¡Œåœ¨ ws://{host}:{port}")
        await websocket_server.wait_closed()
    except Exception as e:
        print(f"âŒ WebSocketæœåŠ¡å™¨é”™è¯¯: {e}")


async def stop_websocket_server():
    """åœæ­¢WebSocketæœåŠ¡å™¨"""
    global websocket_server, server_task
    
    try:
        if websocket_server:
            print("ğŸ›‘ æ­£åœ¨åœæ­¢WebSocketæœåŠ¡å™¨...")
            websocket_server.close()
            await websocket_server.wait_closed()
            websocket_server = None
            print("âœ… WebSocketæœåŠ¡å™¨å·²åœæ­¢")
        
        if server_task:
            server_task.cancel()
            try:
                await server_task
            except asyncio.CancelledError:
                pass
            server_task = None
            
    except Exception as e:
        print(f"âŒ åœæ­¢WebSocketæœåŠ¡å™¨æ—¶å‡ºé”™: {e}")


async def _handle_client_message(websocket, data: Dict[str, Any]):
    """å¤„ç†å®¢æˆ·ç«¯æ¶ˆæ¯"""
    message_type = data.get("type")
    
    if message_type == "ping":
        await websocket.send(json.dumps({
            "type": "pong",
            "timestamp": datetime.now().isoformat()
        }))
    elif message_type == "request_status":
        status = get_audio_status()
        await websocket.send(json.dumps({
            "type": "status_response",
            "data": status
        }))


def _generate_realistic_mock_audio(text: str, voice_style: str) -> bytes:
    """ç”Ÿæˆå¯æ’­æ”¾çš„æ¨¡æ‹ŸéŸ³é¢‘æ•°æ®"""
    
    # åˆ›å»ºä¸€ä¸ªæœ€å°çš„MP3æ–‡ä»¶ç»“æ„
    # è¿™æ˜¯ä¸€ä¸ªéå¸¸ç®€å•çš„MP3æ–‡ä»¶ï¼ŒåŒ…å«çŸ­æš‚çš„é™éŸ³
    
    # MP3æ–‡ä»¶å¤´ï¼ˆID3v2.3æ ‡ç­¾ï¼‰
    id3_header = bytearray([
        0x49, 0x44, 0x33,  # "ID3"
        0x03, 0x00,        # Version 2.3
        0x00,              # Flags
        0x00, 0x00, 0x00, 0x17  # Size (23 bytes)
    ])
    
    # æ ‡é¢˜æ ‡ç­¾
    title_frame = bytearray([
        0x54, 0x49, 0x54, 0x32,  # "TIT2" (Title frame)
        0x00, 0x00, 0x00, 0x0D,  # Frame size (13 bytes)
        0x00, 0x00,              # Flags
        0x00                     # Text encoding (ISO-8859-1)
    ])
    
    # æ ¹æ®è¯­éŸ³é£æ ¼ç”Ÿæˆä¸åŒçš„æ ‡é¢˜
    style_titles = {
        "enthusiastic": b"NHL_ENERGETIC",
        "dramatic": b"NHL_INTENSE", 
        "calm": b"NHL_SMOOTH"
    }
    title_text = style_titles.get(voice_style, b"NHL_NORMAL")
    
    # MP3å¸§å¤´ï¼ˆ128 kbps, 44.1 kHz, Stereoï¼‰
    mp3_frame_header = bytearray([
        0xFF, 0xFB,  # Frame sync + MPEG Audio Layer III
        0x90, 0x00   # Bitrate + Frequency + Padding + Mode
    ])
    
    # åˆ›å»ºçŸ­æš‚çš„é™éŸ³MP3æ•°æ®ï¼ˆçº¦0.1ç§’ï¼‰
    # è¿™ä¸ªæ˜¯ç»è¿‡ç®€åŒ–çš„MP3å¸§æ•°æ®
    silence_data = bytearray([
        0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
        0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
        0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
        0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00
    ] * 4)  # é‡å¤å‡ æ¬¡ä»¥åˆ›å»ºçŸ­æš‚çš„é™éŸ³
    
    # ç»„åˆå®Œæ•´çš„MP3æ–‡ä»¶
    mp3_data = id3_header + title_frame + title_text + mp3_frame_header + silence_data
    
    return bytes(mp3_data)


def _generate_simple_wav_audio(text: str, voice_style: str) -> bytes:
    """ç”Ÿæˆæ›´æ¥è¿‘è¯­éŸ³çš„WAVéŸ³é¢‘æ–‡ä»¶"""
    
    # WAVæ–‡ä»¶å¤´
    sample_rate = 44100
    duration = min(3.0, max(1.0, len(text) * 0.12))  # æ ¹æ®æ–‡æœ¬é•¿åº¦è°ƒæ•´
    num_samples = int(sample_rate * duration)
    
    # RIFFå¤´
    riff_header = b'RIFF'
    file_size = (36 + num_samples * 2).to_bytes(4, 'little')
    wave_header = b'WAVE'
    
    # fmtå­å—
    fmt_header = b'fmt '
    fmt_size = (16).to_bytes(4, 'little')
    audio_format = (1).to_bytes(2, 'little')  # PCM
    num_channels = (1).to_bytes(2, 'little')  # Mono
    sample_rate_bytes = sample_rate.to_bytes(4, 'little')
    byte_rate = (sample_rate * 1 * 2).to_bytes(4, 'little')
    block_align = (1 * 2).to_bytes(2, 'little')
    bits_per_sample = (16).to_bytes(2, 'little')
    
    # dataå­å—
    data_header = b'data'
    data_size = (num_samples * 2).to_bytes(4, 'little')
    
    # è¯­éŸ³é£æ ¼é…ç½® - NHLè§£è¯´å‘˜é£æ ¼
    voice_configs = {
        "enthusiastic": {
            "base_freq": 170,     # å…´å¥‹çš„è§£è¯´å‘˜
            "formants": [850, 1250, 2650],  # å…±æŒ¯å³°
            "pitch_variation": 0.5,
            "amplitude": 7000,
            "speech_rate": 1.4
        },
        "dramatic": {
            "base_freq": 150,     # æˆå‰§æ€§çš„ä½æ²‰å£°éŸ³
            "formants": [750, 1150, 2450],
            "pitch_variation": 0.7,
            "amplitude": 8000,
            "speech_rate": 0.9
        },
        "calm": {
            "base_freq": 130,     # å¹³é™çš„è§£è¯´
            "formants": [650, 1050, 2250],
            "pitch_variation": 0.3,
            "amplitude": 6000,
            "speech_rate": 0.8
        }
    }
    
    config = voice_configs.get(voice_style, voice_configs["enthusiastic"])
    
    # åˆ†ææ–‡æœ¬å†…å®¹è°ƒæ•´è¯­éŸ³ç‰¹å¾
    text_upper = text.upper()
    words = text.split()
    
    # åˆ›å»ºè¯­è°ƒæ¨¡å¼
    pitch_pattern = []
    for word in words:
        word_upper = word.upper()
        if any(x in word_upper for x in ["GOAL", "SCORE", "YES"]):
            pitch_pattern.extend([1.3, 1.6, 1.8, 1.4])  # å…´å¥‹ä¸Šå‡
        elif any(x in word_upper for x in ["SAVE", "BLOCK", "STOP"]):
            pitch_pattern.extend([1.2, 0.7, 1.4, 1.0])  # ç´§å¼ å˜åŒ–
        elif any(x in word_upper for x in ["OVERTIME", "FINAL"]):
            pitch_pattern.extend([1.0, 1.1, 1.0, 0.9])  # ç´§å¼ æ„Ÿ
        else:
            pitch_pattern.extend([1.0 + random.uniform(-0.15, 0.25)])
    
    audio_samples = []
    
    for i in range(num_samples):
        t = i / sample_rate
        progress = t / duration
        
        # è¯­è°ƒå˜åŒ–
        if pitch_pattern:
            pattern_index = int(progress * len(pitch_pattern))
            pattern_index = min(pattern_index, len(pitch_pattern) - 1)
            pitch_mult = pitch_pattern[pattern_index]
        else:
            pitch_mult = 1.0
        
        # åŸºé¢‘è®¡ç®—
        base_freq = config["base_freq"] * pitch_mult
        
        # è¯­è°ƒè½®å»“
        pitch_contour = config["pitch_variation"] * math.sin(2 * math.pi * progress * 3)
        freq = base_freq * (1 + pitch_contour)
        
        # è¯­éŸ³ä¿¡å·ç”Ÿæˆ
        # åŸºé¢‘ï¼ˆå£°å¸¦æŒ¯åŠ¨ï¼‰
        fundamental = math.sin(2 * math.pi * freq * t)
        
        # å…±æŒ¯å³°åˆæˆï¼ˆæ¨¡æ‹Ÿå£è…”å…±é¸£ï¼‰
        formant_signal = 0
        for j, formant_freq in enumerate(config["formants"]):
            formant_strength = 0.8 / (j + 1)  # é€’å‡å¼ºåº¦
            formant_mod = formant_freq * (1 + 0.03 * math.sin(2 * math.pi * t * 4 + j))
            formant_component = formant_strength * math.sin(2 * math.pi * formant_mod * t)
            formant_signal += formant_component
        
        # è¯­éŸ³åˆæˆï¼šåŸºé¢‘è°ƒåˆ¶å…±æŒ¯å³°
        voice_signal = fundamental * (1 + 0.4 * formant_signal)
        
        # æ·»åŠ è¯­éŸ³å™ªå£°æˆåˆ†
        noise_component = 0.08 * random.uniform(-1, 1)
        
        # èŠ‚å¥æ§åˆ¶ï¼šæ¨¡æ‹ŸéŸ³èŠ‚é—´éš”
        syllable_rate = config["speech_rate"] * 4
        syllable_phase = (t * syllable_rate) % 1.0
        syllable_envelope = 0.6 + 0.4 * math.sin(syllable_phase * math.pi)
        
        # æ·»åŠ è½»å¾®çš„åœé¡¿
        if syllable_phase < 0.05:
            syllable_envelope *= 0.2
        
        # æœ€ç»ˆä¿¡å·
        sample_value = (voice_signal + noise_component) * config["amplitude"]
        sample_value *= syllable_envelope
        
        # æ•´ä½“åŒ…ç»œï¼ˆæ·¡å…¥æ·¡å‡ºï¼‰
        envelope = 1.0
        fade_time = 0.1
        if t < fade_time:
            envelope = t / fade_time
        elif t > duration - fade_time:
            envelope = (duration - t) / fade_time
        
        sample_value *= envelope
        
        # è½¬æ¢ä¸º16ä½æ•´æ•°
        sample_int = int(sample_value)
        sample_int = max(-32768, min(32767, sample_int))
        
        # è½¬æ¢ä¸ºå­—èŠ‚
        sample_bytes = sample_int.to_bytes(2, 'little', signed=True)
        audio_samples.append(sample_bytes)
    
    audio_data = b''.join(audio_samples)
    
    # ç»„åˆWAVæ–‡ä»¶
    wav_data = (riff_header + file_size + wave_header + 
                fmt_header + fmt_size + audio_format + num_channels + 
                sample_rate_bytes + byte_rate + block_align + bits_per_sample +
                data_header + data_size + audio_data)
    
    return wav_data


async def save_audio_file(
    tool_context: Optional[ToolContext],
    audio_base64: str,
    audio_id: str,
    voice_style: str = "enthusiastic",
    game_id: str = "unknown"
) -> Dict[str, Any]:
    """
    Save audio data to organized file structure
    
    Args:
        tool_context: ADK tool context (can be None)
        audio_base64: Base64 encoded audio data
        audio_id: Unique audio identifier
        voice_style: Voice style used (enthusiastic, dramatic, calm)
        game_id: NHL game ID for organized storage
        
    Returns:
        Dict with save status and file path
    """
    
    try:
        import base64
        import wave
        import os
        from datetime import datetime
        
        # Create organized output directory
        output_dir = f"audio_output/{game_id}"
        os.makedirs(output_dir, exist_ok=True)
        
        # Decode audio data
        audio_bytes = base64.b64decode(audio_base64)
        
        # Generate filename with timestamp
        timestamp = datetime.now().strftime("%H%M%S")
        filename = f"nhl_{voice_style}_{audio_id}_{timestamp}.wav"
        filepath = os.path.join(output_dir, filename)
        
        # Create WAV file
        with wave.open(filepath, 'wb') as wav_file:
            wav_file.setnchannels(1)      # Mono
            wav_file.setsampwidth(2)      # 16-bit
            wav_file.setframerate(24000)  # 24kHz
            wav_file.writeframes(audio_bytes)
        
        file_size = os.path.getsize(filepath)
        
        print(f"ğŸ’¾ Audio saved: {filepath} ({file_size:,} bytes)")
        
        # Update tool context if available
        if tool_context:
            tool_context.state["last_audio_save"] = {
                "status": "success",
                "filepath": filepath,
                "filename": filename,
                "size": file_size,
                "audio_id": audio_id
            }
        
        return {
            "status": "success",
            "filepath": filepath,
            "filename": filename,
            "size": file_size,
            "audio_id": audio_id,
            "game_id": game_id,
            "voice_style": voice_style,
            "message": f"Audio saved successfully to {filepath}"
        }
        
    except Exception as e:
        error_msg = f"Failed to save audio file: {str(e)}"
        print(f"âŒ {error_msg}")
        
        if tool_context:
            tool_context.state["last_audio_save"] = {
                "status": "error",
                "error": error_msg
            }
        
        return {
            "status": "error",
            "error": error_msg,
            "audio_id": audio_id
        }


# åˆ›å»ºADK FunctionToolå®ä¾‹
text_to_speech_tool = FunctionTool(func=text_to_speech)
stream_audio_tool = FunctionTool(func=stream_audio_websocket)
audio_status_tool = FunctionTool(func=get_audio_status)
save_audio_tool = FunctionTool(func=save_audio_file)

# å¯¼å‡ºå·¥å…·åˆ—è¡¨
AUDIO_TOOLS = [
    text_to_speech_tool,
    stream_audio_tool,
    audio_status_tool,
    save_audio_tool
]
